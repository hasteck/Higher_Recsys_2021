{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nterop": {
     "id": "825"
    }
   },
   "source": [
    "# Negative Interactions for Improved Collaborative Filtering: \n",
    "# Don’t go Deeper, go Higher\n",
    "\n",
    "This notebook provides an implementation in Python 3 of the algorithm outlined in the paper \n",
    "\"Negative Interactions for Improved Collaborative Filtering: Don’t go Deeper, go Higher\" published \n",
    "at the 15th ACM Conference on Recommender Systems(RecSys 2021), Amsterdam, Netherlands.\n",
    "\n",
    "The results of Table 1 in this paper can be reproduced in the following three steps:\n",
    "- Step 1: Pre-processing the data (as in this publicly available [code](https://github.com/dawenl/vae_cf))\n",
    "- Step 2: Loading the pre-processed data, and defining the evaluation-functions (as in this publicly available [code](https://github.com/dawenl/vae_cf))\n",
    "- Step 3: Learning and Evaluating the higher-order model in this paper.\n",
    "\n",
    "We use the same code for pre-processing the data and evaluating the model as was made publicly available in this [code](https://github.com/dawenl/vae_cf)), which accompanies the paper \"[Variational autoencoders for collaborative filtering](https://arxiv.org/abs/1802.05814)\" by Dawen Liang et al. at The Web Conference 2018. \n",
    "While their code for the Movielens-20M data-set was made publicly available, the code for pre-processing the other two data-sets can easily be obtained by modifying their code as described in their paper.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nterop": {
     "id": "826"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "import bottleneck as bn\n",
    "from copy import deepcopy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nterop": {
     "id": "827"
    }
   },
   "source": [
    "## Step 1: Pre-processing the data\n",
    "\n",
    "Exactly like in this [code](https://github.com/dawenl/vae_cf), for the [MovieLens-20M](http://files.grouplens.org/datasets/movielens/ml-20m.zip) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nterop": {
     "id": "828"
    }
   },
   "outputs": [],
   "source": [
    "### change `DATA_DIR` to the location of the dataset\n",
    "DATA_DIR = '/my/data/folder/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nterop": {
     "id": "829"
    }
   },
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(os.path.join(DATA_DIR, 'ratings.csv'), header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nterop": {
     "id": "830"
    }
   },
   "outputs": [],
   "source": [
    "# binarize the data (only keep ratings >= 4)\n",
    "raw_data = raw_data[raw_data['rating'] > 3.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nterop": {
     "id": "831"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>151</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1094785734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>223</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1112485573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>253</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1112484940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>260</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1112484826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>293</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1112484703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    userId  movieId  rating   timestamp\n",
       "6        1      151     4.0  1094785734\n",
       "7        1      223     4.0  1112485573\n",
       "8        1      253     4.0  1112484940\n",
       "9        1      260     4.0  1112484826\n",
       "10       1      293     4.0  1112484703"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "nterop": {
       "id": "832"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nterop": {
     "id": "833"
    }
   },
   "outputs": [],
   "source": [
    "def get_count(tp, id):\n",
    "    playcount_groupbyid = tp[[id]].groupby(id, as_index=False)\n",
    "    count = playcount_groupbyid.size()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nterop": {
     "id": "834"
    }
   },
   "outputs": [],
   "source": [
    "def filter_triplets(tp, min_uc=5, min_sc=0):\n",
    "    # Only keep the triplets for items which were clicked on by at least min_sc users. \n",
    "    if min_sc > 0:\n",
    "        itemcount = get_count(tp, 'movieId')\n",
    "        tp = tp[tp['movieId'].isin(itemcount.index[itemcount >= min_sc])]\n",
    "    \n",
    "    # Only keep the triplets for users who clicked on at least min_uc items\n",
    "    # After doing this, some of the items will have less than min_uc users, but should only be a small proportion\n",
    "    if min_uc > 0:\n",
    "        usercount = get_count(tp, 'userId')\n",
    "        tp = tp[tp['userId'].isin(usercount.index[usercount >= min_uc])]\n",
    "    \n",
    "    # Update both usercount and itemcount after filtering\n",
    "    usercount, itemcount = get_count(tp, 'userId'), get_count(tp, 'movieId') \n",
    "    return tp, usercount, itemcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nterop": {
     "id": "835"
    }
   },
   "outputs": [],
   "source": [
    "raw_data, user_activity, item_popularity = filter_triplets(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nterop": {
     "id": "836"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering, there are 9990682 watching events from 136677 users and 20720 movies (sparsity: 0.353%)\n"
     ]
    }
   ],
   "source": [
    "sparsity = 1. * raw_data.shape[0] / (user_activity.shape[0] * item_popularity.shape[0])\n",
    "print(\"After filtering, there are %d watching events from %d users and %d movies (sparsity: %.3f%%)\" % \n",
    "      (raw_data.shape[0], user_activity.shape[0], item_popularity.shape[0], sparsity * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nterop": {
     "id": "837"
    }
   },
   "outputs": [],
   "source": [
    "unique_uid = user_activity.index\n",
    "\n",
    "np.random.seed(98765)\n",
    "idx_perm = np.random.permutation(unique_uid.size)\n",
    "unique_uid = unique_uid[idx_perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "nterop": {
     "id": "838"
    }
   },
   "outputs": [],
   "source": [
    "### create train/validation/test users\n",
    "n_users = unique_uid.size\n",
    "n_heldout_users = 10000\n",
    "\n",
    "tr_users = unique_uid[:(n_users - n_heldout_users * 2)]\n",
    "vd_users = unique_uid[(n_users - n_heldout_users * 2): (n_users - n_heldout_users)]\n",
    "te_users = unique_uid[(n_users - n_heldout_users):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "nterop": {
     "id": "839"
    }
   },
   "outputs": [],
   "source": [
    "train_plays = raw_data.loc[raw_data['userId'].isin(tr_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "nterop": {
     "id": "840"
    }
   },
   "outputs": [],
   "source": [
    "unique_sid = pd.unique(train_plays['movieId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "nterop": {
     "id": "841"
    }
   },
   "outputs": [],
   "source": [
    "show2id = dict((sid, i) for (i, sid) in enumerate(unique_sid))\n",
    "profile2id = dict((pid, i) for (i, pid) in enumerate(unique_uid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "nterop": {
     "id": "842"
    }
   },
   "outputs": [],
   "source": [
    "pro_dir = os.path.join(DATA_DIR, 'pro_sg')\n",
    "\n",
    "if not os.path.exists(pro_dir):\n",
    "    os.makedirs(pro_dir)\n",
    "\n",
    "with open(os.path.join(pro_dir, 'unique_sid.txt'), 'w') as f:\n",
    "    for sid in unique_sid:\n",
    "        f.write('%s\\n' % sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "nterop": {
     "id": "843"
    }
   },
   "outputs": [],
   "source": [
    "def split_train_test_proportion(data, test_prop=0.2):\n",
    "    data_grouped_by_user = data.groupby('userId')\n",
    "    tr_list, te_list = list(), list()\n",
    "\n",
    "    np.random.seed(98765)\n",
    "\n",
    "    for i, (_, group) in enumerate(data_grouped_by_user):\n",
    "        n_items_u = len(group)\n",
    "\n",
    "        if n_items_u >= 5:\n",
    "            idx = np.zeros(n_items_u, dtype='bool')\n",
    "            idx[np.random.choice(n_items_u, size=int(test_prop * n_items_u), replace=False).astype('int64')] = True\n",
    "\n",
    "            tr_list.append(group[np.logical_not(idx)])\n",
    "            te_list.append(group[idx])\n",
    "        else:\n",
    "            tr_list.append(group)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(\"%d users sampled\" % i)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    data_tr = pd.concat(tr_list)\n",
    "    data_te = pd.concat(te_list)\n",
    "    \n",
    "    return data_tr, data_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "nterop": {
     "id": "844"
    }
   },
   "outputs": [],
   "source": [
    "vad_plays = raw_data.loc[raw_data['userId'].isin(vd_users)]\n",
    "vad_plays = vad_plays.loc[vad_plays['movieId'].isin(unique_sid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "nterop": {
     "id": "845"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 users sampled\n",
      "1000 users sampled\n",
      "2000 users sampled\n",
      "3000 users sampled\n",
      "4000 users sampled\n",
      "5000 users sampled\n",
      "6000 users sampled\n",
      "7000 users sampled\n",
      "8000 users sampled\n",
      "9000 users sampled\n"
     ]
    }
   ],
   "source": [
    "vad_plays_tr, vad_plays_te = split_train_test_proportion(vad_plays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "nterop": {
     "id": "846"
    }
   },
   "outputs": [],
   "source": [
    "test_plays = raw_data.loc[raw_data['userId'].isin(te_users)]\n",
    "test_plays = test_plays.loc[test_plays['movieId'].isin(unique_sid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "nterop": {
     "id": "847"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 users sampled\n",
      "1000 users sampled\n",
      "2000 users sampled\n",
      "3000 users sampled\n",
      "4000 users sampled\n",
      "5000 users sampled\n",
      "6000 users sampled\n",
      "7000 users sampled\n",
      "8000 users sampled\n",
      "9000 users sampled\n"
     ]
    }
   ],
   "source": [
    "test_plays_tr, test_plays_te = split_train_test_proportion(test_plays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "nterop": {
     "id": "848"
    }
   },
   "outputs": [],
   "source": [
    "def numerize(tp):\n",
    "    uid = map(lambda x: profile2id[x], tp['userId'])\n",
    "    sid = map(lambda x: show2id[x], tp['movieId'])\n",
    "    return pd.DataFrame(data={'uid': list(uid), 'sid': list(sid)}, columns=['uid', 'sid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "nterop": {
     "id": "849"
    }
   },
   "outputs": [],
   "source": [
    "train_data = numerize(train_plays)\n",
    "train_data.to_csv(os.path.join(pro_dir, 'train.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "nterop": {
     "id": "850"
    }
   },
   "outputs": [],
   "source": [
    "vad_data_tr = numerize(vad_plays_tr)\n",
    "vad_data_tr.to_csv(os.path.join(pro_dir, 'validation_tr.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "nterop": {
     "id": "851"
    }
   },
   "outputs": [],
   "source": [
    "vad_data_te = numerize(vad_plays_te)\n",
    "vad_data_te.to_csv(os.path.join(pro_dir, 'validation_te.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "nterop": {
     "id": "852"
    }
   },
   "outputs": [],
   "source": [
    "test_data_tr = numerize(test_plays_tr)\n",
    "test_data_tr.to_csv(os.path.join(pro_dir, 'test_tr.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "nterop": {
     "id": "853"
    }
   },
   "outputs": [],
   "source": [
    "test_data_te = numerize(test_plays_te)\n",
    "test_data_te.to_csv(os.path.join(pro_dir, 'test_te.csv'), index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nterop": {
     "id": "854"
    }
   },
   "source": [
    "## Step 2: Load pre-processed data, define the Evaluation Functions\n",
    "As in this [code](https://github.com/dawenl/vae_cf)\n",
    "\n",
    "Load the pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "nterop": {
     "id": "855"
    }
   },
   "outputs": [],
   "source": [
    "unique_sid = list()\n",
    "with open(os.path.join(pro_dir, 'unique_sid.txt'), 'r') as f:\n",
    "    for line in f:\n",
    "        unique_sid.append(line.strip())\n",
    "\n",
    "n_items = len(unique_sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "nterop": {
     "id": "10"
    }
   },
   "outputs": [],
   "source": [
    "def load_train_data(csv_file):\n",
    "    tp = pd.read_csv(csv_file)\n",
    "    n_users = tp['uid'].max() + 1\n",
    "\n",
    "    rows, cols = tp['uid'], tp['sid']\n",
    "    data = sparse.csr_matrix((np.ones_like(rows),\n",
    "                             (rows, cols)), dtype='float64',\n",
    "                             shape=(n_users, n_items))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "nterop": {
     "id": "14"
    }
   },
   "outputs": [],
   "source": [
    "def load_tr_te_data(csv_file_tr, csv_file_te):\n",
    "    tp_tr = pd.read_csv(csv_file_tr)\n",
    "    tp_te = pd.read_csv(csv_file_te)\n",
    "\n",
    "    start_idx = min(tp_tr['uid'].min(), tp_te['uid'].min())\n",
    "    end_idx = max(tp_tr['uid'].max(), tp_te['uid'].max())\n",
    "\n",
    "    rows_tr, cols_tr = tp_tr['uid'] - start_idx, tp_tr['sid']\n",
    "    rows_te, cols_te = tp_te['uid'] - start_idx, tp_te['sid']\n",
    "\n",
    "    data_tr = sparse.csr_matrix((np.ones_like(rows_tr),\n",
    "                             (rows_tr, cols_tr)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))\n",
    "    data_te = sparse.csr_matrix((np.ones_like(rows_te),\n",
    "                             (rows_te, cols_te)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))\n",
    "    return data_tr, data_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "nterop": {
     "id": "856"
    }
   },
   "outputs": [],
   "source": [
    "### load training data\n",
    "X = load_train_data(os.path.join(pro_dir, 'train.csv'))\n",
    "XtX=np.array( ( X.transpose() * X).todense()) \n",
    "XtXdiag=deepcopy(np.diag(XtX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "nterop": {
     "id": "857"
    }
   },
   "outputs": [],
   "source": [
    "### load test data\n",
    "test_data_tr, test_data_te = load_tr_te_data(\n",
    "    os.path.join(pro_dir, 'test_tr.csv'),\n",
    "    os.path.join(pro_dir, 'test_te.csv'))\n",
    "\n",
    "N_test = test_data_tr.shape[0]\n",
    "idxlist_test = range(N_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nterop": {
     "id": "23"
    }
   },
   "source": [
    "Evaluate functions: Normalized discounted cumulative gain (NDCG@k) and Recall@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "nterop": {
     "id": "24"
    }
   },
   "outputs": [],
   "source": [
    "def NDCG_binary_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "    '''\n",
    "    normalized discounted cumulative gain@k for binary relevance\n",
    "    ASSUMPTIONS: all the 0's in heldout_data indicate 0 relevance\n",
    "    '''\n",
    "    batch_users = X_pred.shape[0]\n",
    "    idx_topk_part = bn.argpartition(-X_pred, k, axis=1)\n",
    "    topk_part = X_pred[np.arange(batch_users)[:, np.newaxis],\n",
    "                       idx_topk_part[:, :k]]\n",
    "    idx_part = np.argsort(-topk_part, axis=1)\n",
    "    # X_pred[np.arange(batch_users)[:, np.newaxis], idx_topk] is the sorted\n",
    "    # topk predicted score\n",
    "    idx_topk = idx_topk_part[np.arange(batch_users)[:, np.newaxis], idx_part]\n",
    "    # build the discount template\n",
    "    tp = 1. / np.log2(np.arange(2, k + 2))\n",
    "\n",
    "    DCG = (heldout_batch[np.arange(batch_users)[:, np.newaxis],\n",
    "                         idx_topk].toarray() * tp).sum(axis=1)\n",
    "    IDCG = np.array([(tp[:min(n, k)]).sum()\n",
    "                     for n in heldout_batch.getnnz(axis=1)])\n",
    "    return DCG / IDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "nterop": {
     "id": "25"
    }
   },
   "outputs": [],
   "source": [
    "def Recall_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "    batch_users = X_pred.shape[0]\n",
    "\n",
    "    idx = bn.argpartition(-X_pred, k, axis=1)\n",
    "    X_pred_binary = np.zeros_like(X_pred, dtype=bool)\n",
    "    X_pred_binary[np.arange(batch_users)[:, np.newaxis], idx[:, :k]] = True\n",
    "\n",
    "    X_true_binary = (heldout_batch > 0).toarray()\n",
    "    tmp = (np.logical_and(X_true_binary, X_pred_binary).sum(axis=1)).astype(\n",
    "        np.float32)\n",
    "    recall = tmp / np.minimum(k, X_true_binary.sum(axis=1))\n",
    "    return recall"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nterop": {
     "id": "858"
    }
   },
   "source": [
    "## Step 3: Training and Evaluation of Higher-order Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "nterop": {
     "id": "859"
    }
   },
   "outputs": [],
   "source": [
    "### functions to create the feature-pairs\n",
    "def create_list_feature_pairs(XtX, threshold):\n",
    "    AA= np.triu(np.abs(XtX))\n",
    "    AA[ np.diag_indices(AA.shape[0]) ]=0.0\n",
    "    ii_pairs = np.where((AA>threshold)==True)\n",
    "    return ii_pairs\n",
    "\n",
    "def create_matrix_Z(ii_pairs, X):\n",
    "    MM = np.zeros( (len(ii_pairs[0]), X.shape[1]),    dtype=np.float)\n",
    "    MM[np.arange(MM.shape[0]) , ii_pairs[0]   ]=1.0\n",
    "    MM[np.arange(MM.shape[0]) , ii_pairs[1]   ]=1.0\n",
    "    CCmask = 1.0-MM    # see Eq. 8 in the paper\n",
    "    MM=sparse.csc_matrix(MM.T)\n",
    "    Z=  X * MM\n",
    "    Z= (Z == 2.0 )\n",
    "    Z=Z*1.0\n",
    "    return [ Z, CCmask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "nterop": {
     "id": "860"
    }
   },
   "outputs": [],
   "source": [
    "### training-function of higher-order model\n",
    "def train_higher(XtX, XtXdiag,lambdaBB, ZtZ, ZtZdiag, lambdaCC, CCmask, ZtX, rho, epochs):\n",
    "    # precompute for BB\n",
    "    ii_diag=np.diag_indices(XtX.shape[0])\n",
    "    XtX[ii_diag] = XtXdiag+lambdaBB\n",
    "    PP=np.linalg.inv(XtX)\n",
    "    # precompute for CC\n",
    "    ii_diag_ZZ=np.diag_indices(ZtZ.shape[0])\n",
    "    ZtZ[ii_diag_ZZ] = ZtZdiag+lambdaCC+rho\n",
    "    QQ=np.linalg.inv(ZtZ)\n",
    "    # initialize\n",
    "    CC = np.zeros( (ZtZ.shape[0], XtX.shape[0]),dtype=np.float )\n",
    "    DD = np.zeros( (ZtZ.shape[0], XtX.shape[0]),dtype=np.float )\n",
    "    UU = np.zeros( (ZtZ.shape[0], XtX.shape[0]),dtype=np.float ) # is Gamma in paper\n",
    "    for iter in range(epochs):\n",
    "        print(\"epoch {}\".format(iter))\n",
    "        # learn BB\n",
    "        XtX[ii_diag] = XtXdiag\n",
    "        BB= PP.dot(XtX-ZtX.T.dot(CC))\n",
    "        gamma = np.diag(BB) / np.diag(PP)\n",
    "        BB-= PP *gamma\n",
    "        # learn CC\n",
    "        CC= QQ.dot(ZtX-ZtX.dot(BB) +rho *(DD-UU))\n",
    "        # learn DD\n",
    "        DD=  CC  * CCmask \n",
    "        #DD= np.maximum(0.0, DD) # if you want to enforce non-negative parameters\n",
    "        # learn UU (is Gamma in paper)\n",
    "        UU+= CC-DD\n",
    "    return [BB,DD]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nterop": {
     "id": "77"
    }
   },
   "source": [
    "train the higher-order model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "nterop": {
     "id": "861"
    }
   },
   "outputs": [],
   "source": [
    "### choose the training-hyperparameters\n",
    "epochs = 40\n",
    "\n",
    "#threshold, lambdaBB, lambdaCC, rho =   1750,  500, 10000, 100000  # ML-20M: 40k of higher orders\n",
    "threshold, lambdaBB, lambdaCC, rho =   3500,  500,  5000, 100000  # ML-20M: 10k of higher orders\n",
    "#threshold, lambdaBB, lambdaCC, rho =   6500,  500,  5000, 100000  # ML-20M:  2k of higher orders\n",
    "#threshold, lambdaBB, lambdaCC, rho =  10000,  500,  2000,  30000  # ML-20M: 500 of higher orders\n",
    "\n",
    "#threshold, lambdaBB, lambdaCC, rho =  13000, 1000, 30000, 100000  # Nflx: 40k of higher orders\n",
    "#threshold, lambdaBB, lambdaCC, rho =  22000, 1000, 30000, 100000  # Nflx: 10k of higher orders\n",
    "#threshold, lambdaBB, lambdaCC, rho =  33000, 1000, 10000, 100000  # Nflx:  2k of higher orders\n",
    "#threshold, lambdaBB, lambdaCC, rho =  44000, 1000,  3000,  30000  # Nflx: 500 of higher orders\n",
    "\n",
    "#threshold, lambdaBB, lambdaCC, rho =    750,  200,  1200,  10000  # MSD: 40k of higher orders\n",
    "#threshold, lambdaBB, lambdaCC, rho =   1850,  200,  1000,  10000  # MSD: 10k of higher orders\n",
    "#threshold, lambdaBB, lambdaCC, rho =   4050,  200,   200,  10000  # MSD:  2k of higher orders\n",
    "#threshold, lambdaBB, lambdaCC, rho =   6820,  200,  1200,  10000  # MSD: 500 of higher orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "nterop": {
     "id": "862"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of feature-pairs: 9997\n"
     ]
    }
   ],
   "source": [
    "### create the list of feature-pairs and the higher-order matrix Z\n",
    "XtX[ np.diag_indices(XtX.shape[0]) ]=XtXdiag #if code is re-run, ensure that the diagonal is correct\n",
    "ii_feature_pairs = create_list_feature_pairs(XtX, threshold)\n",
    "print(\"number of feature-pairs: {}\".format(len(ii_feature_pairs[0])))\n",
    "Z, CCmask = create_matrix_Z(ii_feature_pairs, X)\n",
    "Z_test_data_tr , _ = create_matrix_Z(ii_feature_pairs, test_data_tr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "nterop": {
     "id": "863"
    }
   },
   "outputs": [],
   "source": [
    "### create the higher-order matrices\n",
    "ZtZ=np.array(  (Z.transpose() * Z).todense()) \n",
    "ZtX=np.array( (Z.transpose() * X).todense()) \n",
    "ZtZdiag=deepcopy(np.diag(ZtZ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "nterop": {
     "id": "864"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n",
      "epoch 10\n",
      "epoch 11\n",
      "epoch 12\n",
      "epoch 13\n",
      "epoch 14\n",
      "epoch 15\n",
      "epoch 16\n",
      "epoch 17\n",
      "epoch 18\n",
      "epoch 19\n",
      "epoch 20\n",
      "epoch 21\n",
      "epoch 22\n",
      "epoch 23\n",
      "epoch 24\n",
      "epoch 25\n",
      "epoch 26\n",
      "epoch 27\n",
      "epoch 28\n",
      "epoch 29\n",
      "epoch 30\n",
      "epoch 31\n",
      "epoch 32\n",
      "epoch 33\n",
      "epoch 34\n",
      "epoch 35\n",
      "epoch 36\n",
      "epoch 37\n",
      "epoch 38\n",
      "epoch 39\n"
     ]
    }
   ],
   "source": [
    "### iterative training, and evaluation every 10 epochs \n",
    "BB, CC = train_higher(XtX, XtXdiag, lambdaBB, ZtZ, ZtZdiag, lambdaCC, CCmask, ZtX, rho, epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nterop": {
     "id": "865"
    }
   },
   "source": [
    "evaluate the higher-order model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "nterop": {
     "id": "866"
    }
   },
   "outputs": [],
   "source": [
    "### evaluation-function of higher-order model\n",
    "def evaluate_higher(BB,CC,test_data_tr, Z_test_data_tr, N_test, batch_size_test=5000):\n",
    "    print(\"Evaluating on test set ...\")\n",
    "    #evaluate in batches\n",
    "    n100_list, r20_list, r50_list, r10_list = [], [], [], []\n",
    "    for bnum, st_idx in enumerate(range(0, N_test, batch_size_test)):\n",
    "        end_idx = min(st_idx + batch_size_test, N_test)\n",
    "        Xtest = test_data_tr[idxlist_test[st_idx:end_idx]]\n",
    "        Ztest = Z_test_data_tr[idxlist_test[st_idx:end_idx]]\n",
    "        if sparse.isspmatrix(Xtest):\n",
    "            Xtest = Xtest.toarray()\n",
    "            Ztest = Ztest.toarray()\n",
    "        Xtest = Xtest.astype('float32')\n",
    "        Ztest = Ztest.astype('float32')\n",
    "        pred_val = (Xtest).dot(BB) + Ztest.dot(CC)\n",
    "        pred_val[Xtest.nonzero()] = -np.inf # exclude examples from training and validation (if any)\n",
    "        n100_list.append(NDCG_binary_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=100))\n",
    "        r20_list.append(Recall_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=20))\n",
    "        r50_list.append(Recall_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=50))\n",
    "        r10_list.append(Recall_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=10))\n",
    "    n100_list = np.concatenate(n100_list)\n",
    "    r20_list = np.concatenate(r20_list)\n",
    "    r50_list = np.concatenate(r50_list)\n",
    "    r10_list = np.concatenate(r10_list)\n",
    "    print(\"Test Recall@10=%.5f (%.5f)\" % (np.mean(r10_list), np.std(r10_list) / np.sqrt(len(r10_list))))\n",
    "    print(\"Test Recall@20=%.5f (%.5f)\" % (np.mean(r20_list), np.std(r20_list) / np.sqrt(len(r20_list))))\n",
    "    print(\"Test Recall@50=%.5f (%.5f)\" % (np.mean(r50_list), np.std(r50_list) / np.sqrt(len(r50_list))))\n",
    "    print(\"Test NDCG@100=%.5f (%.5f)\" % (np.mean(n100_list), np.std(n100_list) / np.sqrt(len(n100_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "nterop": {
     "id": "867"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on test set ...\n",
      "Test Recall@10=0.34310 (0.00264)\n",
      "Test Recall@20=0.39999 (0.00270)\n",
      "Test Recall@50=0.53025 (0.00282)\n",
      "Test NDCG@100=0.42923 (0.00215)\n"
     ]
    }
   ],
   "source": [
    "evaluate_higher(BB,CC,test_data_tr, Z_test_data_tr, N_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nterop": {
     "id": "868"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "nterop": {
   "seedId": "868"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
